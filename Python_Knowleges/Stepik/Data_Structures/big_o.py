"""
Какие структуры данных бывают:
- статические и динамические массивы;
- стек;
- очередь;
- односвязный и двусвязный список;
- множество;
- хэш-таблица;
- деревья, бинарные деревья.

Верхняя оценка сложности.
Но зачем это всё нужно? Дело в том что не существует единого универсального подхода представления и хранения данных в
программе, для каждой конкретной задачи следует выбирать свой эффективный способ организации данных в памяти компьютера.
Чтобы уметь это делать и следует знать эти основные структуры данных.
Но для начала нужно рассмотреть, с какой скоростью работают алгоритмы. Есть несколько вариантов:
- скорость работы программы по числу арифметических операций - некорректно для всех программ, кроме тех, что будут либо
очень маленькими, либо которые будут взаимодействовать только с математикой;
- скорость работы программы по её фактическому времени выполнения - также некорректно для всех программ, потому что один
и тот же код может выполняться с разной скоростью из-за отличий железа, также будет некорректным потому, что не
учитывается размер входных данных, который может быть разным;
- скорость работы программы как верхняя граница динамики изменения вычислительной сложности алгоритма в зависимости от
размера входных данных - т.е. сложность работы зависит от количества проделанных операций вне зависимости от входных
данных массива, а также скорости железа. Метрика известна как О большое (Big O). Хорошим примером является медленная
сортировка, которая выполняется за O(n^2). Мы видим, что при увеличении массива, сложность операций над ним растёт
в квадрате.

Сложность выполнения константных операций.
Допустим, мы переменной присваиваем имя, потом преобразуем её в другой тип данных, после чего выводим на экран. Каждая
операция выполняется за постоянное время (пусть каждая команда выполняется за 1 мс, но она разная на разном железе).
Какова вычислительная сложность каждой команды? В каждой строчке выполняется 1 операция, которая проходит за константное
время. В нотации большого О это записывается так: O(1) - при этом время не учитывается, здесь единица служит
обозначением того, что скорость выполнения данной операции постоянная. Таким образом, любая операция, даже будь записана
как О(10), будет О(1), т.е. 10 как бы выносится за скобку. Таким образом, для наших трёх операций (присваивания,
изменения и вывода) скорость будет О(1+1+1) = О(3) = О(1).

Алгоритмы с линейной сложностью.
Допустим у нас есть массив из n элементов. Затем мы их последовательно в одном цикле перебираем и выводим на экран.
Какова сложность этого алгоритма? Ясно, что число итераций зависит от размерности массива (чем больше n, тем больше
итераций надо сделать циклу). Т.е. цикл зависит от размера входных данных. Нотация О большого должна показывать динамику
этой зависимости. Т.е. изменение сложности алгоритма от массива входных данных. В данном случае O(n). Стоит обратить
внимание, что n - это уже переменная, а не константа, т.к. в общем случае при работе с массивами и другими
упорядоченными коллекциями, мы полагаем, что n может бесконечно увеличиваться. Всегда стоит помнить, что O большое
всегда показывает верхнюю границу, т.е. худший случай работы алгоритма в вычислительном плане. В записи O(n) хорошо
видна линейная зависимость от сложности - сложность соразмерно увеличивается при увеличении размера массива, т.е.
увеличение идёт 1:1. Также нотация О не зависит от константных операций при оценке сложности в n, ибо мы падаем в
пределы математической погрешности. Так что, как и ранее, константа отбрасывается и остаётся просто O(n). Аналогично
при объявлении переменной массиву мы не будем считать O(1+n), а просто запишем O(n). Фактически нотация большого О
фиксирует динамику роста сложности, а не фактическое время выполнения. Таким образом можно описать формулой работу с
константами при сложностях с n:
A, C - константы:
O(A*n) = O(n)
O(A*n+C) = O(n)
O(n+C) = O(n)

Правила сложения и умножения.
Допустим есть 2 цикла, в которых m и n является переменными (не константами):

for y in range(n):
    print(y)
for x in range(m):
    print(x)

Какую сложность мы получили? O(n) + O(m) = O(n+m) - их нельзя друг из друга вычесть, ибо n и m стремятся к
бесконечности, а отбрасывать можно только константы. Переменные, которые связаны с размерностью входных данных,
отбрасывать нельзя.

А что мы получим, если записать один цикл внутрь другого?

for x in range(n):
    for y in range(m):
        print(x, y)

Какой здесь объём вычислений? С точки зрения О большого - O(n) * O(n) = O(n*m). Умножение получается от того, что цикл
находится в цикле. Т.е. при отработке программы мы пробежим полностью по циклу m n-ное кол-во раз.
Таким образом, для циклов вложенных мы имеем операцию умножения, а для последовательно идущих - сложения.
Также следует рассмотреть, когда n = m. При сложении мы получаем O(2n), двойку мы отбрасываем, так что финальный итог
O(n), а для умножения мы имеем O(n^2). И степень мы уже отбросить не можем.

Итоги:
- О(1) - команды, выполняющиеся за фиксированное время;
- O(n) - алгоритмы с линейной сложностью (например, перебор элементов в цикле);
- O(n^2) - алгоритмы с квадратичной сложностью (например, вложенные циклы).
- Любые константы могут быть отброшены.

Случаи логарифмической и факториальной сложности.
Неважная сложность.

for x in range(n):
    for y in range(n):
        print(x, y)
for y in range(n):
    print(y)

В данном случае мы имеем сложность O(n^2 + n). И мы отбрасываем последнюю n, ибо она имеет линейную сложность.
Квадратичная сложность является в разы худшей, нежели линейная. А по правилам арифметической погрешности у нас линейная
n идёт как погрешность (при условии, что n стремится к бесконечности, то n^2 в разы хуже обычного n). Так что текущая
сложность будет O(n^2).
Но тогда какие факторы являются значимыми, а какие нет?
Полагают, что если одно слагаемое может принимать значение более чем в два раза превышающее другое, значит, оно
значимое, а второе незначимое.

Алгоритмы с логарифмической сложностью.
Пример такого алгоритма - это алгоритм бинарного поиска. Кратко работает так: если мы имеем отсортированный список
чисел, и требуется найти какое-либо значение в этом списке, то можно воспользоваться методом деления на половины. Какова
же максимальная сложность этого алгоритма? Количество максимально возможного деления массива (т.е. худший случай), будет
O(log(n)). Основание логарифма - 2 - это константа и не указывается для простоты, ибо всегда одинаковая.

Алгоритмы с факториальной сложностью.
Следующе распространённые алгоритмы - это NP-полные задачи, в которых требуется проводить полный перебор всех вариантов,
чтобы найти искомое решение. Например, мы хотим написать алгоритм по сбору пазла. В самом простом случае требуется
перебрать все варианты и выбрать нужный. Если число фрагментов пазла обозначить через n, то общее число вариаций их
перебора составить факториал от n = n! = 1 * 2 * 3 * ... * n. Если у нас всего 2 фрагмента, то мы получаем 2 варианта
(2 фрагмента поочерёдно меняются местами) 2! = 1 * 2 = 2. Если фрагмента 3, то 3! = 1 * 2 * 3 = 6. Соответственно,
вычислительная сложность таких алгоритмов составляет O(n!). Ещё одним примером задачи такой задачи является задача
коммивояжера. В ней необходимо выстроить наикратчайший маршрут из вектора начальной точки и пройти по всем городам, так,
чтобы общий суммарный маршрут был как можно короче. Если число городов принять за n, то необходимо перебрать n!
вариантов, что соответствует сложности O(n!). Причём на сегодняшний день эта задача не имеет быстрого алгоритма. Т.е.
для нахождения наилучшего решения требуется произвести полный перебор всех вариантов. Проблема таких задач в том, что
факториал растёт очень быстро, а значит, это одни из самых сложных алгоритмов в вычислительном плане. Как только мы
получили оценку сложности О(n!), есть смысл подумать о более простых подходах, дающих пусть и неточное, но близкое к
точному решение. Конечно, если n < 10 в таких задачах, то ещё можно сделать полный перебор.

Порядок возрастания сложности алгоритмов по возрастанию:
- О(1);
- O(log(n));
- O(n);
- O(n * log(n));
- O(n^2);
- O(2^n);
- O(n!).
Создавать алгоритмы стоит так, чтобы рост входных параметров как можно слабее влиял на количество операций (сложность).
"""
